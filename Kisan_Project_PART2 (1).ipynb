{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIcJATTMsPzD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.util import ngrams\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import seaborn as sns\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan = pd.read_csv('/content/Kisan Call center Queries.csv',on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "jDhB_Y9tsT2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YiM4IuBsier",
        "outputId": "f4a72423-82d7-4c11-f136-386be536d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e51b4aa0622f>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  Kisan['questions'].fillna(' ',inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan['answers'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "X = vectorizer.fit_transform(Kisan['answers'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvSpMlfDsoB2",
        "outputId": "f635df0e-0295-43f9-de3b-27c6cab2917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-b4613c996a1f>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  Kisan['answers'].fillna(' ',inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to numerical representation using TF-IDF\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "X_reduced = svd.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "WZCN1fEysq4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply DBSCAN clustering\n",
        "eps = 1.8  # Adjust based on dataset density\n",
        "min_samples = 10  # Minimum points to form a cluster\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "Kisan['Cluster'] = dbscan.fit_predict(X_reduced)\n"
      ],
      "metadata": {
        "id": "OA_waVXFst1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze clusters by creating word clouds\n",
        "unique_clusters = set(Kisan['Cluster'])\n",
        "for cluster in unique_clusters:\n",
        "    if cluster == -1:\n",
        "        continue  # Skip noise points\n",
        "    cluster_text = \" \".join(Kisan[Kisan['Cluster'] == cluster]['questions'])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Cluster {cluster} Word Cloud\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HD9BkI2uswYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample questions from each cluster\n",
        "for cluster in unique_clusters:\n",
        "    if cluster == -1:\n",
        "        print(\"Noise Points (Outliers):\")\n",
        "    else:\n",
        "        print(f\"Cluster {cluster} Sample Questions:\")\n",
        "    print(Kisan[Kisan['Cluster'] == cluster]['questions'].head(5).to_string(index=False))\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "qXIy1a8Jsy15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Topic Modeling (LDA)"
      ],
      "metadata": {
        "id": "Zlvl6LnHs4aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])"
      ],
      "metadata": {
        "id": "ZADytdXqugY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan['answers'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "X = vectorizer.fit_transform(Kisan['answers'])"
      ],
      "metadata": {
        "id": "7eDy4a8buo5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
        "num_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=num_topics,random_state=42)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "VciFWiA6utUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Display top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx,topic in enumerate(lda.components):\n",
        "  print(f\"Topic {topic_idx}:\")\n",
        "  top_words_idx = topic.argsort()[:10:-1]\n",
        "  top_words_idx = topic.argsort()[:11:-1]\n",
        "  top_words_idx = [feature_names[i] for i in top_words_idx]\n",
        "  print(\",\".join(top_words))\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "XW-dG18BvB4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topics using word clouds\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    word_freqs = {feature_names[i]: topic[i] for i in topic.argsort()[:-51:-1]}  # Top 50 words\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Topic {topic_idx} Word Cloud\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "W-V5XNGzwR2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modeling (NMF)"
      ],
      "metadata": {
        "id": "mO7VJdesw-2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])"
      ],
      "metadata": {
        "id": "kl7qencsxU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan['answers'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "X = vectorizer.fit_transform(Kisan['answers'])"
      ],
      "metadata": {
        "id": "o3EhVrgyxXGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
        "num_topics = 5\n",
        "nmf = NMF(n_components=num_topics,random_state=42,init='nndsvd')\n",
        "nmf.fit(X)\n",
        "\n",
        "# Apply Latent Dirichlet Allocation (LDA) for topic modeling\n",
        "num_topics = 5\n",
        "lda = LatentDirichletAllocation(n_components=num_topics,random_state=42)\n",
        "lda"
      ],
      "metadata": {
        "id": "HOT1iR_HxaK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Display top words for each topic\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "    print(f\"Topic {topic_idx}:\")\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]  # Top 10 words\n",
        "    print(\" \".join(top_words))\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "G3KT0hprx3HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topics using word clouds\n",
        "for topic_idx, topic in enumerate(nmf.components_):\n",
        "    word_freqs = {feature_names[i]: topic[i] for i in topic.argsort()[:-51:-1]}  # Top 50 words\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freqs)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Topic {topic_idx} Word Cloud\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "acGifMa1x_YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Anomaly Detection Model"
      ],
      "metadata": {
        "id": "xkgBxGhYyrA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])"
      ],
      "metadata": {
        "id": "XGLpUi_ZytZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan['answers'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "X = vectorizer.fit_transform(Kisan['answers'])"
      ],
      "metadata": {
        "id": "I90vUCPmy_vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan['text_length'] = Kisan['questions'].apply(len)\n",
        "X = Kisan[['text_length']]\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "QC6mv2YOzB8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iso_forest = IsolationForest(n_estimators=100,contamination=0.08,random_state=42)\n",
        "Kisan['anomaly'] = iso_forest.fit_predict(X_scaled)"
      ],
      "metadata": {
        "id": "40iJFMkKzn2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(Kisan.index, Kisan['text_length'], c=Kisan['anomaly'], cmap='coolwarm', edgecolors='k')\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Text Length\")\n",
        "plt.title(\"Anomaly Detection in Text Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CU9zkNSWz-CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = Kisan[Kisan['anomaly'] == -1]\n",
        "print(\"Detected Anomalies:\")\n",
        "print(anomalies[['questions', 'text_length']].head(10).to_string(index=False))"
      ],
      "metadata": {
        "id": "6J7zPM9T0SPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apriori Algorithm"
      ],
      "metadata": {
        "id": "qE7rUs5iEV3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDWuq4EWEXyl",
        "outputId": "c2c673b1-3483-427d-ae82-0d01cb534361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e51b4aa0622f>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  Kisan['questions'].fillna(' ',inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Kisan_cleaned = Kisan.dropna(subset=['questions'])\n",
        "Kisan_cleaned['questions'] = Kisan_cleaned['questions'].str.lower().str.split\n"
      ],
      "metadata": {
        "id": "euEHzEvGFA-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = Kisan_cleaned['questions'].tolist()"
      ],
      "metadata": {
        "id": "kRX5gJMZFaZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rhLzP3bFjBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "Kisan['questions'].fillna(' ',inplace=True)\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_features=5000)\n",
        "x = vectorizer.fit_transform(Kisan['questions'])\n",
        "\n",
        "Kisan_cleaned = Kisan.dropna(subset=['questions'])\n",
        "# Apply str.lower() and str.split() to the 'questions' column\n",
        "# str.split() is called with no arguments to split on whitespace\n",
        "# Tokenizing questions into individual words (as items for Apriori)\n",
        "Kisan_cleaned['questions'] = Kisan_cleaned['questions'].str.lower().str.split()\n",
        "\n",
        "transactions = Kisan_cleaned['questions'].tolist()\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "Kisan_encoded = pd.DataFrame(te_ary,columns=te.columns_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkvnVzduF4La",
        "outputId": "020dc70f-ad48-48bf-b4af-03eebf4077a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-12c26d4f7b11>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  Kisan['questions'].fillna(' ',inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the Apriori algorithm with a minimum support threshold\n",
        "frequent_itemsets = apriori(Kisan_encoded,min_support=0.01,use_colnames=True)"
      ],
      "metadata": {
        "id": "pvS3KbJIGc2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate association rules with a minimum confidence threshold\n",
        "rules = association_rules(frequent_itemsets,metric='confidence',min_threshold=0.5)\n",
        "print(rules[['antecedents','consequents','support','confidence','lift']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qeK-genG41H",
        "outputId": "4a3f3074-3a79-467a-a19f-24d3b2f7ff30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     antecedents consequents   support  confidence      lift\n",
            "0            (a)     (about)  0.017179    0.569998  0.980343\n",
            "1            (a)    (asking)  0.019414    0.644168  0.986468\n",
            "2            (a)        (of)  0.022483    0.745967  1.532240\n",
            "3            (a)       (the)  0.017179    0.569998  1.736261\n",
            "4          (and)     (about)  0.020895    0.586142  1.008109\n",
            "5  (application)     (about)  0.010842    0.501681  0.862844\n",
            "6        (asked)     (about)  0.018705    0.828670  1.425235\n",
            "7       (asking)     (about)  0.545666    0.835624  1.437195\n",
            "8        (about)    (asking)  0.545666    0.938495  1.437195\n",
            "9    (bacterial)     (about)  0.012155    0.616322  1.060016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eyBP4ZHiHTaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}